# -*- coding: utf-8 -*-
"""NLP03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XRYj9VfM7SOuscIMXsXG5lbO5_qlHJ45
"""

#23(林)
import re
section_list = []#最後に入るリスト
pattern =re.compile(r"^==+.+==+$",re.MULTILINE) #"="×1回以上＋複数文字+"="×1回以上を含む
for s in pattern.findall(text):#"==セクション名=="のリスト
    t = re.subn('=','', s) #"="を""に置き換える→(置換後の文字列,置換した回数)というタプルが出力される
    section_list.append({re.sub(r"\s","",t[0]):int(t[1]/2-1)}) #半角スペースをなくして辞書を作る

print(section_list)

# 補足 下川
import gzip
import json
import re

file = 'jawiki-country.json.gz'

#正規表現
# ^ 行頭
# $ 行末
# . 任意の一文字
# * 0回以上の繰り返し
# () グループ化
# *? 非貪欲マッチ　後ろの空白を考慮するため非貪欲マッチ
# ?: 非マッチ
# \ エスケープ
# \s 空白文字
# re.MULTILINE 各行の先頭末尾にマッチ
# re.findall()マッチする部分をすべてをリストで取得

pattern = re.compile(r'^(={2,})\s*(.*?)\s*(={2,})$', re.MULTILINE)
with gzip.open(file, 'rt') as data:
    for line in data:
        json_data = json.loads(line)
        if json_data['title'] == "イギリス":
            result = re.findall(pattern, json_data["text"])
            for sec in result:
                # =の数-1がレベルになる
                print("{}...{}".format(sec[1], len(sec[0])-1))
            break

#25(林)
import re
templete_dict = dict()
pattern =re.compile(r"\{\{基礎情報.*^\}\}$",re.MULTILINE|re.DOTALL) #"{{基礎情報"+改行も含む複数文字+"}}"
pattern2 = re.compile(r"\|.*=.*\n") #"|"+複数文字+"="+複数文字+"\n"のみの列
templete_list = pattern2.findall(pattern.search(text).group()) #1つのみ探す→.group()でマッチした文字列が出力される
for t in templete_list:
    tlist = t.split(" = ")
    templete_dict[tlist[0].lstrip("|")] = tlist[1].rstrip("\n")
print(templete_dict)

#26 井原
for (k,v) in template_dict.items():
    template_dict[k] = re.sub(r'\'','',v)
print(template_dict)

#27 井原
for (k,v) in template_dict.items():
    template_dict[k] = re.sub(r'(\[\[)|(\]\])','',v)
print(template_dict)

#28 ひがさ
import gzip
import json
import re
file_name = "jawiki-country.json.gz"
#基本情報
def find_field(string):
     return(re.match(r'^\|(.*?)\s+=\s*(.*?)$', string))

def find_field_value(string):
    m=find_field(string)
    return (m.group(1) ,m.group(2))
#内部リンク
def find_field_link(string):
    return(re.match(r'^(.*)\[\[(.*)\|(.*)\]\](.*?)$',string))
#<ref>
def find_markup_ref(string):
    return(re.match(r'^(.*?)<ref(.*?)$', string))
#<br>
def find_markup_br(string):
    return(re.match(r'^(.*?)<(.*?)>(.*?)$',string))
#|
def find_pipe(string):
    return(re.match(r'^.*\|(.*)$',string))
#{{
def a(string):
    return(re.match(r'^(.*){{.*\s+(.*)$',string ))

text = extract_article(file_name,'イギリス').split('\n')
field_dict={}
for line in text:
    #基本情報であること
    if find_field(line):
        #強調マークアップを取り除く
        line = re.sub("'{2,5}" , ' ' , line)
        #keyとvalueにフィールド名と値を入力
        key,value = find_field_value(line)
        
        #  <br>を消去
        if find_markup_br(value):
            m2 = find_markup(value)
            value = m2.group(1) 

        #valueについて当てはまりを調べる
        if find_field_link(value):
            m = find_field_link(value)
            #[[a|b]]のb部分を取り出す
            value = m.group(1)+m.group(3)+m.group(4)
        
        #内部リンクを除去
        value = re.sub(r"\[\[|\]\]",' ',value)
        
        #<ref>を消去
        if find_markup_ref(value):
            m1 = find_markup(value)
            value = m1.group(1)
       
        #pipeを消去
        if find_pipe(value):
            m3 = find_pipe(value)
            value = m3.group(1)
        
        #}}を消去
        value = re.sub(r"}}",' ',value)
        
        #微調整
        if a(value):
            m4 = a(value)
            value = m4.group(1)+m4.group(2)
        if a(value):
            m4 = a(value)
            value = m4.group(1)+m4.group(2)
        
        field_dict[key] = value
        print(key,':',value)

#29 ひがさ
import urllib.parse, urllib.request
import json

url = 'https://www.mediawiki.org/w/api.php?' \
+ 'action=query' \
+ '&format=json' \
+ '&prop=imageinfo' \
+ '&titles=File:' + urllib.parse.quote(field_dict['国旗画像']) \
+ '&iiprop=url'

# MediaWikiのサービスへリクエスト送信
connection = urllib.request.urlopen(url)

# jsonとして受信
data = json.loads(connection.read().decode('utf-8'))

# URL取り出し
url = data['query']['pages']['-1']['imageinfo'][0]['url']
print(url)

import requests

S = requests.Session()

URL = "https://en.wikipedia.org/w/api.php"

PARAMS = {
    "action": "query",
    "format": "json",
    "prop": "imageinfo",
    "titles": "File:"+template["国旗画像"],
    "iiprop": "url"
}

R = S.get(url=URL, params=PARAMS)
DATA = R.json()

print(list(DATA['query']['pages'].values())[0]['imageinfo'][0]['url'])